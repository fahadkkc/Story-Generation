{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "story_generation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "qegEhDUZDofO"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qegEhDUZDofO"
      },
      "source": [
        "---\n",
        "# PREPARATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSW4SBNiKNjK"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import layers, Model\n",
        "import os\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "import string\n",
        "import re"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWZkhNsB9HkQ"
      },
      "source": [
        "def save_dataset(dataset,fileName):\n",
        "  path = os.path.join('./tfDatasets/', fileName)\n",
        "  tf.data.experimental.save(dataset, path)\n",
        "\n",
        "def load_dataset(fileName):\n",
        "  path = os.path.join(\"./tfDatasets/\", fileName)\n",
        "  new_dataset = tf.data.experimental.load(path,\n",
        "      tf.TensorSpec(shape=(), dtype=tf.string))\n",
        "  return new_dataset"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ww_QSwXTD5JD"
      },
      "source": [
        "---\n",
        "# Building an Efficient TensorFlow Input Pipeline  for Character-Level Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LEEtF_H3hei"
      },
      "source": [
        "To load the data into our pipeline, I will use `tf.data.TextLineDataset()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTbmeG0AK2Bd"
      },
      "source": [
        "batch_size = 64\n",
        "#raw_data_ds = tf.data.TextLineDataset([\"nietzsche.txt\"])\n",
        "raw_data_ds = tf.data.TextLineDataset([\"story.txt\"])"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcPuxgGp3wIm"
      },
      "source": [
        "Let's see some lines from the uploaded text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjG06bfK4r5W",
        "outputId": "c7234ec2-e750-4f02-c0ff-696f2356aa94"
      },
      "source": [
        "for elems in raw_data_ds.take(10):\n",
        "    print(elems.numpy().decode(\"utf-8\"))"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rick grew up in a troubled household . he never found good support in family , and turned to gangs . it was n't long before rick got shot in a robbery . the incident caused him to turn a new leaf .\n",
            "laverne needs to prepare something for her friend 's party . she decides to bake a batch of brownies . she chooses a recipe and follows it closely . laverne tests one of the brownies to make sure it is delicious .\n",
            "sarah had been dreaming of visiting europe for years . she had finally saved enough for the trip . she landed in spain and traveled east across the continent . she did n't like how different everything was .\n",
            "gina was worried the cookie dough in the tube would be gross . she was very happy to find she was wrong . the cookies from the tube were as good as from scratch . gina intended to only eat 2 cookies and save the rest .\n",
            "it was my final performance in marching band . i was playing the snare drum in the band . we played thriller and radar love . the performance was flawless .\n",
            "i had been giving this homeless man change everyday . he was on the same corner near my house . one day , as i was driving through my neighborhood i saw a new car . soon enough , i saw the same homeless man emerge from it !\n",
            "jim found an old disposable camera in the bottom of his junk drawer . he began snapping away at everything around him . the counter clicked down to one final photo . the gravity of the situation began to dawn on jim .\n",
            "ron started his new job as a landscaper today . he loves the outdoors and has always enjoyed working in it . his boss tells him to re-sod the front yard of the mayor 's home . ron is ecstatic , but does a thorough job and finishes super early .\n",
            "john and billy became very skilled at beer pong . they entered a contest in college . they won the contest and advanced to the next level . the next level sent them to vegas .\n",
            "caroline was a student in medical school . caroline worked very hard to get good grades . one day caroline failed a test by one point . caroline was very frustrated but she continued to study hard .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qRIcg58tEaB"
      },
      "source": [
        "---\n",
        "# 3. COMBINE ALL LINES INTO A SINGLE TEXT "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnuB_2MF3-gJ"
      },
      "source": [
        "Since our aim is to prepare a train dataset for a **character-level** text generator, we need to **convert the line-by-line text into char-by-char text**. \n",
        "\n",
        "Therefore, we first combine all line-by-line text as **a single text**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kfAU3OORcD2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8de87412-15dd-469f-9846-d4b144c24c4b"
      },
      "source": [
        "text=\"\"\n",
        "for elem in raw_data_ds:\n",
        "   text=text+(elem.numpy().decode('utf-8'))\n",
        "\n",
        "\n",
        "print(text[:1000])"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rick grew up in a troubled household . he never found good support in family , and turned to gangs . it was n't long before rick got shot in a robbery . the incident caused him to turn a new leaf .laverne needs to prepare something for her friend 's party . she decides to bake a batch of brownies . she chooses a recipe and follows it closely . laverne tests one of the brownies to make sure it is delicious .sarah had been dreaming of visiting europe for years . she had finally saved enough for the trip . she landed in spain and traveled east across the continent . she did n't like how different everything was .gina was worried the cookie dough in the tube would be gross . she was very happy to find she was wrong . the cookies from the tube were as good as from scratch . gina intended to only eat 2 cookies and save the rest .it was my final performance in marching band . i was playing the snare drum in the band . we played thriller and radar love . the performance was flawless .i had bee\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNTCFQGgs8El"
      },
      "source": [
        "---\n",
        "# 4. SPLIT THE TEXT INTO TOKENS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZpe5HaPsF2a"
      },
      "source": [
        "## Check the size of the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3L2i2Kc2vBKy",
        "outputId": "1fd90793-7475-430e-cff1-1f2f073b6675"
      },
      "source": [
        "print(\"Corpus length:\", int(len(text)/1000),\"K chars\")"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus length: 516 K chars\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0zg8dmexEwa",
        "outputId": "f25a8855-7a40-4efb-a8b9-0e9913ecfeaa"
      },
      "source": [
        "chars = sorted(list(set(text)))\n",
        "print(\"Total disctinct chars:\", len(chars))"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total disctinct chars: 85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlvkNf5x_qz9"
      },
      "source": [
        "## Set the split parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcRlxqwBxvUG"
      },
      "source": [
        "We can split the text into two sets of **fixed-size char sequences** as below:\n",
        "* The first sequence (**`input_chars`**) is  the **input data** (X) to the model which will receive a fixed-size (**`maxlen`**) character sequence \n",
        "* The second sequence (**`next_char`**) is  the **output data** (y) to the model which is only  1 char \n",
        "\n",
        "While creating these sequences, we can jump over the data by setting **`step`** to a fixed character number.\n",
        "\n",
        "We define all these parameters below:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXpEadsc-g4i"
      },
      "source": [
        "# cut the text in semi-redundant sequences of maxlen characters\n",
        "maxlen = 20\n",
        "step = 3\n",
        "input_chars = []\n",
        "next_char = []"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9KIL6AjECkl"
      },
      "source": [
        "Using the above parameters, we can split the text into **input (X)** and **output (y)** sequences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0EX2eWsEApl"
      },
      "source": [
        "for i in range(0, len(text) - maxlen, step):\n",
        "    input_chars.append(text[i : i + maxlen])\n",
        "    next_char.append(text[i + maxlen])"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG2mhWqDHwBL"
      },
      "source": [
        "## Check the generated sequences\n",
        "After splitting the text, we can check the number of sequences and see a sample input and output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JW0DEHgHj6x",
        "outputId": "000e43de-77f8-4aa1-8ff9-0f3fdfe44540"
      },
      "source": [
        "print(\"Number of sequences:\", len(input_chars))\n",
        "print(\"input X  (input_chars)  --->   output y (next_char) \")\n",
        "\n",
        "for i in range(5):\n",
        "  print( input_chars[i],\"   --->  \", next_char[i])\n",
        "\n"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sequences: 172247\n",
            "input X  (input_chars)  --->   output y (next_char) \n",
            "rick grew up in a tr    --->   o\n",
            "k grew up in a troub    --->   l\n",
            "rew up in a troubled    --->    \n",
            " up in a troubled ho    --->   u\n",
            " in a troubled house    --->   h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tdYeZR0yBBD"
      },
      "source": [
        "---\n",
        "# 5. CREATE X & y DATASETS\n",
        "\n",
        "We can use these two sequences to create **X and y datasets** by using **`tf.data.Dataset.from_tensor_slices()`** method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJqnQbP9_VtV"
      },
      "source": [
        "X_train_ds_raw=tf.data.Dataset.from_tensor_slices(input_chars)\n",
        "y_train_ds_raw=tf.data.Dataset.from_tensor_slices(next_char)"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_gXIOxmE3ZE"
      },
      "source": [
        "Let's see some input-output pairs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LuJy9SdEuJj",
        "outputId": "f195ed50-adc4-4d2e-9132-aa57461cd58d"
      },
      "source": [
        "for elem1, elem2 in zip(X_train_ds_raw.take(5),y_train_ds_raw.take(5)):\n",
        "   print(elem1.numpy().decode('utf-8'),\"----->\", elem2.numpy().decode('utf-8'))"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rick grew up in a tr -----> o\n",
            "k grew up in a troub -----> l\n",
            "rew up in a troubled ----->  \n",
            " up in a troubled ho -----> u\n",
            " in a troubled house -----> h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CX2SLkaoznRU"
      },
      "source": [
        "---\n",
        "# 6. PREPROCESS THE TEXT\n",
        "\n",
        "We need to process these datasets before feeding them into a model. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxrMwBb-7ZDI"
      },
      "source": [
        "## What are the preprocessing steps?\n",
        "\n",
        "The processing of each sample contains the following steps:\n",
        "\n",
        "* **standardize** each sample (usually lowercasing + punctuation stripping): \n",
        "\n",
        "  In this tutorial, we will create a **custom standardization function** to show how to apply your code to strip un-wanted chars and symbols.\n",
        "\n",
        "* **split** each sample into substrings (usually words):\n",
        "\n",
        "  As in this part, we choose to split the text into **fixed-size character sequences**, we will write a **custom split function**\n",
        "\n",
        "* **recombine** substrings into tokens (usually ngrams):\n",
        "  We will leave it as 1 ngram (char)\n",
        "\n",
        "* **index tokens** (associate a unique int value with each token)\n",
        "\n",
        "* **transform** each sample using this index, either into a vector of ints or a dense float vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_cxwMjOGdtT"
      },
      "source": [
        "def custom_standardization(input_data):\n",
        "    lowercase     = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
        "    stripped_num  = tf.strings.regex_replace(stripped_html, \"[\\d-]\", \" \")\n",
        "    stripped_punc  =tf.strings.regex_replace(stripped_num, \n",
        "                             \"[%s]\" % re.escape(string.punctuation), \"\")    \n",
        "    return stripped_punc\n",
        "\n",
        "def char_split(input_data):\n",
        "  return tf.strings.unicode_split(input_data, 'UTF-8')\n",
        "\n",
        "def word_split(input_data):\n",
        "  return tf.strings.split(input_data)"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqWF-KSGHfO6"
      },
      "source": [
        "### Set the text vectorization parameters\n",
        "\n",
        "* We can limit the number of distinct characters by setting `max_features`\n",
        "* We set an explicit `sequence_length`, since our  model needs **fixed-size** input sequences.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0olhhwkmHpWA"
      },
      "source": [
        "# Model constants.\n",
        "max_features = 96           # Number of distinct chars / words  \n",
        "embedding_dim = 16             # Embedding layer output dimension\n",
        "sequence_length = maxlen       # Input sequence size\n"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jh-jwbHGaOc"
      },
      "source": [
        "### Create the text vectorization layer\n",
        "\n",
        "* The **text vectorization layer** is initialized below. \n",
        "* We are using this layer to normalize, split, and map strings to integers, so we set our 'output_mode' to '**int**'.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbaD2D2g9Um0"
      },
      "source": [
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    split=char_split, # word_split or char_split\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDKrIWje8gYB"
      },
      "source": [
        "### Adapt the Text Vectorization layer to the train dataset\n",
        "\n",
        "Now that the **Text Vectorization layer** has been created, we can call `adapt` on a text-only dataset to create the vocabulary with indexing. \n",
        "\n",
        "You don't have to batch, but for very large datasets this means you're not keeping spare copies of the dataset in memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBADpXSI5CEo"
      },
      "source": [
        "vectorize_layer.adapt(X_train_ds_raw.batch(batch_size))"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGo5h1vU9XfU"
      },
      "source": [
        "We can take a look at the size of the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpjeAUQP_4R5",
        "outputId": "cd592eb0-50f2-4109-849e-51b8bc343695"
      },
      "source": [
        "print(\"The size of the vocabulary (number of distinct characters): \", len(vectorize_layer.get_vocabulary()))"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The size of the vocabulary (number of distinct characters):  29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oljh8AL_96M5"
      },
      "source": [
        "Let's see the first 5 entries in the vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4skkz3cRt9L",
        "outputId": "95c0e585-3070-4743-df38-764a675ff3ab"
      },
      "source": [
        "print(\"The first 10 entries: \", vectorize_layer.get_vocabulary()[:10])"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first 10 entries:  ['', '[UNK]', ' ', 'e', 't', 'a', 'o', 'h', 'i', 'n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc5px6Vf-2Fu"
      },
      "source": [
        "You can access the vocabulary by using an index:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "5EzlUbQKQDE9",
        "outputId": "a170f736-5df6-4bb1-ed98-8fc5b52630ca"
      },
      "source": [
        "vectorize_layer.get_vocabulary()[3]"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'e'"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B67x-BRl-NR3"
      },
      "source": [
        "After preparing the **Text Vectorization layer**,  we need a helper function to **convert a given raw text to a Tensor** by using this layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EHkAlit9vkD"
      },
      "source": [
        "def vectorize_text(text):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return tf.squeeze(vectorize_layer(text))"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiQ-OAti_HXi"
      },
      "source": [
        "A simple test of the function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJbqVuuN90a7",
        "outputId": "cb99a9e0-2642-497b-a0ec-0a0e75dc76a3"
      },
      "source": [
        "vectorize_text(\"Ne ister gönül?\")"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(20,), dtype=int64, numpy=\n",
              "array([ 9,  3,  2,  8, 10,  4,  3, 11,  2, 17,  1,  9,  1, 13,  0,  0,  0,\n",
              "        0,  0,  0])>"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt0Wu9OL_NOf"
      },
      "source": [
        "### Apply the **Text Vectorization** onto X and y datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qelaS3UgBijo",
        "outputId": "d0bc0e40-d57b-48ea-e5ee-edf6e0ce6792"
      },
      "source": [
        "# Vectorize the data.\n",
        "X_train_ds = X_train_ds_raw.map(vectorize_text)\n",
        "y_train_ds = y_train_ds_raw.map(vectorize_text)\n",
        "\n",
        "X_train_ds.element_spec, y_train_ds.element_spec"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(20,), dtype=tf.int64, name=None),\n",
              " TensorSpec(shape=(20,), dtype=tf.int64, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAxDin0K_vkG"
      },
      "source": [
        "### Convert **y** to a single char representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfqc5DmR4r5t"
      },
      "source": [
        "y_train_ds=y_train_ds.map(lambda x: x[0])"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95FwQIDd4r5u",
        "outputId": "913f5388-7911-4ec6-ecdb-120bdb001fee"
      },
      "source": [
        "for elem in y_train_ds.take(1):\n",
        "  print(\"shape: \", elem.shape, \"\\n next_char: \",elem.numpy())"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape:  () \n",
            " next_char:  6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4p1OyUU_9-6"
      },
      "source": [
        "### Check the tensor dimensions to ensure that we have max-sequence size inputs and a single output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ER-uQmQAHCUa",
        "outputId": "db8ca708-b6b3-44dd-ba23-8b775c1e94b4"
      },
      "source": [
        "X_train_ds.take(1), y_train_ds.take(1)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<TakeDataset element_spec=TensorSpec(shape=(20,), dtype=tf.int64, name=None)>,\n",
              " <TakeDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>)"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUaJQmgaO3_X"
      },
      "source": [
        "### Let's see an example pair:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-cG6d87Ocaq",
        "outputId": "30ffb7f3-3c8f-462f-af55-a9290cf500d0"
      },
      "source": [
        "for (X,y) in zip(X_train_ds.take(5), y_train_ds.take(5)):\n",
        "  print(X.numpy(),\" --> \",y.numpy())"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11  8 16 23  2 17 11  3 14  2 15 21  2  8  9  2  5  2  4 11]  -->  6\n",
            "[23  2 17 11  3 14  2 15 21  2  8  9  2  5  2  4 11  6 15 22]  -->  13\n",
            "[11  3 14  2 15 21  2  8  9  2  5  2  4 11  6 15 22 13  3 12]  -->  2\n",
            "[ 2 15 21  2  8  9  2  5  2  4 11  6 15 22 13  3 12  2  7  6]  -->  15\n",
            "[ 2  8  9  2  5  2  4 11  6 15 22 13  3 12  2  7  6 15 10  3]  -->  7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxxIr2TJASCR"
      },
      "source": [
        "# 7. FINALIZE THE DATA PIPELINE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTZmg1LYAfR1"
      },
      "source": [
        "## Join the input (X) and output (y) values as a single dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufWe5SSbAeRJ"
      },
      "source": [
        "train_ds =  tf.data.Dataset.zip((X_train_ds,y_train_ds))"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlbQPmTSAu9o"
      },
      "source": [
        "## Set data pipeline optimizations\n",
        "Do async prefetching / buffering of the data for best performance on GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0_gdj6OxWuP"
      },
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.shuffle(buffer_size=512).batch(batch_size, drop_remainder=True).cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_25puVsA1Rw"
      },
      "source": [
        "## Check the size of the dataset (in batches):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pP2dn0NdMd1h",
        "outputId": "aa3c61d8-d105-4560-befc-348913cd2358"
      },
      "source": [
        "print(\"The size of the dataset (in batches)): \", train_ds.cardinality().numpy())"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The size of the dataset (in batches)):  2691\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OIl3qZ4BZgn"
      },
      "source": [
        "## Again, let's check the tensor dimensions of **input X** and **output y**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uc6K8t0q4r50",
        "outputId": "16e2af49-f29f-476a-963a-58764d2bd862"
      },
      "source": [
        "for sample in train_ds.take(1):\n",
        "  print(\"input (X) dimension: \", sample[0].numpy().shape, \"\\noutput (y) dimension: \",sample[1].numpy().shape)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input (X) dimension:  (64, 20) \n",
            "output (y) dimension:  (64,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_WXIofieTHi"
      },
      "source": [
        "## Basic LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsOKHPLbOwn2",
        "outputId": "0a04818b-ba6d-493b-b3a2-5949516826a3"
      },
      "source": [
        "# define model \n",
        "# A integer input for vocab indices.\n",
        "inputs = tf.keras.Input(shape=(sequence_length), dtype=\"int64\")\n",
        "# Next, we add a layer to map those vocab indices into a space of dimensionality\n",
        "# 'embedding_dim'.\n",
        "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
        "#x = layers.Dropout(0.5)(x)\n",
        "x = layers.LSTM(128, return_sequences=True)(x)\n",
        "x = layers.Flatten()(x)\n",
        "predictions=  layers.Dense(max_features, activation='softmax')(x)\n",
        "model_LSTM = tf.keras.Model(inputs, predictions,name=\"model_LSTM\")\n",
        "#sequence_length\n",
        "# compile model\n",
        "model_LSTM.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model_LSTM.summary())\n"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_LSTM\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 20)]              0         \n",
            "                                                                 \n",
            " embedding_2 (Embedding)     (None, 20, 16)            1536      \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 20, 128)           74240     \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 2560)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 96)                245856    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 321,632\n",
            "Trainable params: 321,632\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HNYURebUBt3",
        "outputId": "1a9c4e59-a36d-4cc0-bf69-64672adbe035"
      },
      "source": [
        "model_LSTM.fit(train_ds, epochs=3) #, validation_data= train_ds.skip(20000)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "2691/2691 [==============================] - 141s 44ms/step - loss: 2.5756 - accuracy: 0.2679\n",
            "Epoch 2/3\n",
            "2691/2691 [==============================] - 93s 35ms/step - loss: 2.1331 - accuracy: 0.3666\n",
            "Epoch 3/3\n",
            "2691/2691 [==============================] - 94s 35ms/step - loss: 1.9366 - accuracy: 0.4243\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f536c9c7f50>"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mv3aE83Az9S8",
        "outputId": "7b31287f-41ae-4d13-8004-ec2ce228ea50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model_LSTM.save(\"model_LSTM\")"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: model_LSTM/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: model_LSTM/assets\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f5372766750> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfuCl3na4r59"
      },
      "source": [
        "model_LSTM = tf.keras.models.load_model('model_LSTM')"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BySwtAH4x9ry",
        "outputId": "0614c1aa-6cce-442d-fa6b-8609ced98000",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vectorize_text(\"Ayrılıktan parça parça olmuş\")\n"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(20,), dtype=int64, numpy=\n",
              "array([ 5, 19, 11,  1, 13,  1, 23,  4,  5,  9,  2, 21,  5, 11,  1,  5,  2,\n",
              "       21,  5, 11])>"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PLTqp5x4r6A"
      },
      "source": [
        "def sample(preds, temperature=0.2):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds=np.squeeze(preds)\n",
        "    \n",
        "    preds = np.asarray(preds).astype(\"float64\")\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmu8JbXP13kG"
      },
      "source": [
        "def generate_text(model, seed_original, step,seed):\n",
        "    seed= vectorize_text(seed_original)\n",
        "    decode_sentence(seed.numpy().squeeze())\n",
        "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "        print(\"...Diversity:\", diversity)\n",
        "        seed= vectorize_text(seed_original).numpy().reshape(1,-1)\n",
        "        \n",
        "\n",
        "        generated = (seed)\n",
        "        for i in range(step):\n",
        "            #print(seed.shape)\n",
        "            predictions=model.predict(seed)\n",
        "            pred_max= np.argmax(predictions.squeeze())\n",
        "            #print(\"pred_max: \", pred_max)\n",
        "            next_index = sample(predictions, diversity)\n",
        "            #print(\"next_index: \", next_index)\n",
        "            generated = np.append(generated, next_index)\n",
        "            seed= generated[-sequence_length:].reshape(1,sequence_length)\n",
        "        decode_sentence(generated)\n",
        "    \n",
        "\n"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZD77UyC1lN5"
      },
      "source": [
        "def decode_sentence (encoded_sentence):\n",
        "  deceoded_sentence=[]\n",
        "  for word in encoded_sentence:\n",
        "    \n",
        "    deceoded_sentence.append(vectorize_layer.get_vocabulary()[word])\n",
        "  sentence= ''.join(deceoded_sentence)\n",
        "  print(sentence)\n",
        "  return sentence\n"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed=str(input(\"Enter the words:\\n \"))\n",
        "seed=np.array(list(seed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsCCPIkEiKwj",
        "outputId": "c01b0658-87b0-4927-a1e4-e2f752af6735"
      },
      "execution_count": 139,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the words:\n",
            " alice project knife broken\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fILBboHX4r6O",
        "outputId": "50de4fe7-6d7a-4b43-c8c5-42b632365fbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "generate_text(model_LSTM, \"iteration: \",100,seed)"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration \n",
            "...Diversity: 0.2\n",
            "iteration  foud of out of the was a sone the was so porect the work and the sonter the was to a lontire and th\n",
            "...Diversity: 0.5\n",
            "iteration ficy of to eeaun a off the torteraar the dostert of the fromect gutent th t me jorsatite ang to te\n",
            "...Diversity: 1.0\n",
            "iteration  ofole of ennear or ton of asdrows of ithoupe his rejearents rlast in wad in a nama a ducuuns to om \n",
            "...Diversity: 1.2\n",
            "iteration  thik barcutenefes cromscatssencickt a anc tume cutonk in thenik wonkouts ouf of thillkinw wastee\n"
          ]
        }
      ]
    }
  ]
}